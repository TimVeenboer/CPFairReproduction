{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import yaml\n",
    "import random as rd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_yaml(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        return yaml.safe_load(f)\n",
    "\n",
    "ds = read_yaml(file_path = \"configs.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_config = \"MovieLens100K\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(f\"datasets/{ds[ds_config]['ds_name']}/{ds[ds_config]['ds_name']}_train.txt\", sep='\\t', names=['uid', 'iid', 'rating'])\n",
    "test_data = pd.read_csv(f\"datasets/{ds[ds_config]['ds_name']}/{ds[ds_config]['ds_name']}_test.txt\", sep='\\t', names=['uid', 'iid', 'rating'])\n",
    "long_tail_items = pd.read_csv(f\"datasets/{ds[ds_config]['ds_name']}/groups/items/020/longtail_items.txt\", names=['iid'])\n",
    "short_head_items = pd.read_csv(f\"datasets/{ds[ds_config]['ds_name']}/groups/items/020/shorthead_items.txt\", names=['iid'])\n",
    "active_users = pd.read_csv(f\"datasets/{ds[ds_config]['ds_name']}/groups/users/005/active_ids.txt\", names=['uid'])\n",
    "inactive_users = pd.read_csv(f\"datasets/{ds[ds_config]['ds_name']}/groups/users/005/inactive_ids.txt\", names=['uid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_ids = train_data['uid'].unique()\n",
    "items_ids = train_data['iid'].unique()\n",
    "num_users = len(train_data['uid'].unique())\n",
    "num_items = len(train_data['iid'].unique())\n",
    "active_users_list = list(active_users['uid'].unique())\n",
    "inactive_users_list = list(inactive_users['uid'].unique())\n",
    "longtail_items_list = list(long_tail_items['iid'].unique())\n",
    "shorthead_items_list = list(short_head_items['iid'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_items = dict()\n",
    "\n",
    "for eachline in train_data.iterrows():\n",
    "    uid, iid = eachline[1][0], eachline[1][0]\n",
    "    uid, iid = int(uid), int(iid)\n",
    "    # a dictionary of popularity of items\n",
    "    if iid in pop_items.keys():\n",
    "      pop_items[iid] += 1\n",
    "    else:\n",
    "      pop_items[iid] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precisionk(actual, predicted):\n",
    "  return 1.0 * len(set(actual) & set(predicted)) / len(predicted)\n",
    "\n",
    "def recallk(actual, predicted):\n",
    "  return 1.0 * len(set(actual) & set(predicted)) / len(actual)\n",
    "\n",
    "def ndcgk(actual, predicted):\n",
    "  idcg = 1.0\n",
    "  dcg = 1.0 if predicted[0] in actual else 0.0\n",
    "  for i, p in enumerate(predicted[1:]):\n",
    "    if p in actual:\n",
    "      dcg += 1.0 / np.log(i+2)\n",
    "    idcg += 1.0 / np.log(i+2)\n",
    "  return dcg / idcg\n",
    "\n",
    "def novelty(predicted: list, pop: dict, u: int, k: int) -> float:\n",
    "  self_information = 0\n",
    "  for item in predicted:\n",
    "    if item in pop.keys():\n",
    "      item_popularity = pop[item] / u\n",
    "      item_novelty_value = np.sum(-np.log2(item_popularity))\n",
    "    else:\n",
    "      item_novelty_value = 0\n",
    "    self_information += item_novelty_value\n",
    "  novelty_score = self_information / k\n",
    "  return novelty_score\n",
    "\n",
    "def catalog_coverage(predicted: list, catalog: list) -> float:\n",
    "  predicted_flattened = [p for sublist in predicted for p in sublist]\n",
    "  L_predictions = len(set(predicted_flattened))\n",
    "  catalog_coverage = round(L_predictions / (len(catalog) * 1.0), 2)\n",
    "  return catalog_coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read test data\n",
    "def read_ground_truth():\n",
    "  ground_truth = defaultdict(set)\n",
    "  for eachline in test_data.iterrows():\n",
    "    uid, iid = eachline[1][0], eachline[1][1]\n",
    "    uid, iid = int(uid), int(iid)\n",
    "    ground_truth[uid].add(iid)\n",
    "  return ground_truth\n",
    "\n",
    "ground_truth = read_ground_truth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_per_group(group):\n",
    "  # group: a list of users ids\n",
    "\n",
    "  NDCG10 = list()\n",
    "  Pre10 = list()\n",
    "  Rec10 = list()\n",
    "  Novelty10 = list()\n",
    "  predicted = list()\n",
    "  All_Predicted = list()\n",
    "\n",
    "  long_items = 0\n",
    "  short_items = 0\n",
    "  all_items = 0\n",
    "\n",
    "  for uid in tqdm(group):\n",
    "    if uid in ground_truth.keys():\n",
    "      # print(uid)\n",
    "      for item in top_n_random[uid]:\n",
    "        all_items += 1\n",
    "        if item in longtail_items_list:\n",
    "          long_items += 1\n",
    "        if item in shorthead_items_list:\n",
    "          short_items += 1\n",
    "        predicted.append(item)\n",
    "\n",
    "      copy_predicted = predicted[:] \n",
    "      All_Predicted.append(copy_predicted)\n",
    "      NDCG = ndcgk(actual=ground_truth[uid], predicted=predicted)\n",
    "      Pre = precisionk(actual=ground_truth[uid], predicted=predicted)\n",
    "      Rec = recallk(actual=ground_truth[uid], predicted=predicted)\n",
    "      Novelty = novelty(predicted=predicted, pop=pop_items, u=num_users, k=10)\n",
    "\n",
    "      NDCG10.append(NDCG)\n",
    "      Pre10.append(Pre)\n",
    "      Rec10.append(Rec)\n",
    "      Novelty10.append(Novelty)\n",
    "    \n",
    "      # cleaning the predicted list for a new user\n",
    "      predicted.clear()\n",
    "\n",
    "  catalog = catalog_coverage(predicted=All_Predicted, catalog=pop_items.keys())\n",
    "  return round(np.mean(NDCG10), 5), round(np.mean(Pre10), 5), round(np.mean(Rec10), 5), round(np.mean(Novelty10), 5), catalog, short_items, long_items, all_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random model is selected:\n"
     ]
    }
   ],
   "source": [
    "def get_top_n_random(n=10):\n",
    "  print(\"Random model is selected:\")\n",
    "  top_n = defaultdict(list)\n",
    "  items = list(items_ids)\n",
    "  for uid in users_ids:\n",
    "    if uid not in top_n.keys():\n",
    "      # user_id = list(rs.train_set.user_ids)[uid]\n",
    "      for i in range(0, n):\n",
    "        top_n[int(uid)].append(int(rd.choice(items)))\n",
    "  return top_n\n",
    "\n",
    "top_n_random = get_top_n_random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [00:00<00:00, 512.12it/s]\n",
      "100%|██████████| 896/896 [00:01<00:00, 514.00it/s]\n",
      "100%|██████████| 943/943 [00:01<00:00, 517.70it/s]\n"
     ]
    }
   ],
   "source": [
    "result_file = open(f\"{ds_config}_random.txt\", \"w\")\n",
    "for groups in [('active', active_users_list), ('inactive', inactive_users_list), ('all', users_ids)]:\n",
    "    results = metric_per_group(group=groups[1])\n",
    "    result_file.write(f\"{groups[0]}: {results}\\n\")\n",
    "result_file.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0adcc2737ebf6a4a119f135174df96668767fca1ef1112612db5ecadf2b6d608"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
